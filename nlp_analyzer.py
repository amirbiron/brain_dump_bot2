"""
××•×“×•×œ NLP ×œ× ×™×ª×•×— ××—×©×‘×•×ª
××–×”×” ×§×˜×’×•×¨×™×•×ª, × ×•×©××™× ×•××™×œ×•×ª ××¤×ª×— ×‘×˜×§×¡×˜ ×¢×‘×¨×™
"""

from typing import Dict, List, Set
import re
import logging
from config import CATEGORIES, TOPICS

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NLPAnalyzer:
    """
    ××—×œ×§×” ×œ× ×™×ª×•×— ×˜×§×¡×˜ ×•×–×™×”×•×™ ×§×˜×’×•×¨×™×•×ª/× ×•×©××™×
    """
    
    def __init__(self):
        """××ª×—×•×œ ×”×× ×ª×—"""
        self.categories = CATEGORIES
        self.topics = TOPICS
        
        # ×‘× ×™×™×ª ××™×œ×•× ×™× ×œ×—×™×¤×•×© ××”×™×¨
        self._build_lookup_tables()
    
    def _build_lookup_tables(self):
        """
        ×‘× ×™×™×ª ×˜×‘×œ××•×ª lookup ×œ×‘×™×¦×•×¢×™× ×˜×•×‘×™× ×™×•×ª×¨
        """
        # ×”××¨×ª ×›×œ ×”×˜×¨×™×’×¨×™× ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
        self.category_triggers = {}
        for category, data in self.categories.items():
            self.category_triggers[category] = [
                trigger.lower() for trigger in data["triggers"]
            ]
        
        # ×”××¨×ª ×›×œ keywords ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
        self.topic_keywords = {}
        for topic, data in self.topics.items():
            self.topic_keywords[topic] = [
                keyword.lower() for keyword in data["keywords"]
            ]
    
    def analyze(self, text: str) -> Dict:
        """
        × ×™×ª×•×— ××œ× ×©×œ ×˜×§×¡×˜
        
        Args:
            text: ×”×˜×§×¡×˜ ×œ× ×™×ª×•×—
        
        Returns:
            ××™×œ×•×Ÿ ×¢× ×ª×•×¦××•×ª ×”× ×™×ª×•×—:
            {
                "category": str,
                "topics": List[str],
                "keywords": List[str],
                "sentiment": str,
                "confidence": float
            }
        """
        if not text or not text.strip():
            return self._empty_analysis()
        
        # × ×™×¨××•×œ ×”×˜×§×¡×˜
        normalized_text = self._normalize_text(text)
        
        # ×–×™×”×•×™ ×§×˜×’×•×¨×™×”
        category, category_confidence = self._detect_category(normalized_text)
        
        # ×–×™×”×•×™ × ×•×©××™×
        topics = self._detect_topics(normalized_text)
        
        # ×—×™×œ×•×¥ ××™×œ×•×ª ××¤×ª×—
        keywords = self._extract_keywords(normalized_text)
        
        # × ×™×ª×•×— ×¨×’×© ×‘×¡×™×¡×™
        sentiment = self._basic_sentiment_analysis(normalized_text)
        
        analysis = {
            "category": category,
            "topics": topics,
            "keywords": keywords,
            "sentiment": sentiment,
            "confidence": category_confidence
        }
        
        logger.info(f"ğŸ“Š × ×™×ª×•×— ×”×•×©×œ×: ×§×˜×’×•×¨×™×”={category}, × ×•×©××™×={topics}")
        
        return analysis
    
    def _normalize_text(self, text: str) -> str:
        """
        × ×™×¨××•×œ ×˜×§×¡×˜ - ×”×¡×¨×ª ×¨×•×•×—×™× ××™×•×ª×¨×™×, ×”××¨×” ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
        
        Args:
            text: ×˜×§×¡×˜ ××§×•×¨×™
        
        Returns:
            ×˜×§×¡×˜ ×× ×•×¨××œ
        """
        # ×”×¡×¨×ª ×¨×•×•×—×™× ××™×•×ª×¨×™×
        text = re.sub(r'\s+', ' ', text)
        
        # ×”××¨×” ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
        text = text.lower().strip()
        
        return text
    
    def _detect_category(self, text: str) -> tuple[str, float]:
        """
        ×–×™×”×•×™ ×”×§×˜×’×•×¨×™×” ×”××ª××™××” ×‘×™×•×ª×¨
        
        Args:
            text: ×˜×§×¡×˜ ×× ×•×¨××œ
        
        Returns:
            (×©×_×§×˜×’×•×¨×™×”, ×¨××ª_×‘×™×˜×—×•×Ÿ)
        """
        category_scores = {}
        
        # ×—×™×©×•×‘ ×¦×™×•×Ÿ ×œ×›×œ ×§×˜×’×•×¨×™×”
        for category, triggers in self.category_triggers.items():
            score = 0
            matches = []
            
            for trigger in triggers:
                # ×—×™×¤×•×© ×”××™×œ×”/×‘×™×˜×•×™ ×‘×˜×§×¡×˜
                if self._word_in_text(trigger, text):
                    score += 1
                    matches.append(trigger)
            
            if score > 0:
                category_scores[category] = {
                    "score": score,
                    "matches": matches
                }
        
        # ×× ×œ× × ××¦××” ×§×˜×’×•×¨×™×” - ×‘×¨×™×¨×ª ××—×“×œ
        if not category_scores:
            return "×”×¨×”×•×¨×™×", 0.3
        
        # ××¦×™××ª ×”×§×˜×’×•×¨×™×” ×¢× ×”×¦×™×•×Ÿ ×”×’×‘×•×” ×‘×™×•×ª×¨
        best_category = max(
            category_scores.items(),
            key=lambda x: x[1]["score"]
        )
        
        category_name = best_category[0]
        score = best_category[1]["score"]
        
        # ×—×™×©×•×‘ confidence (0-1)
        # ×›×›×œ ×©×™×•×ª×¨ ×˜×¨×™×’×¨×™× - ×‘×™×˜×—×•×Ÿ ×’×‘×•×” ×™×•×ª×¨
        confidence = min(score / 3.0, 1.0)  # ××§×¡×™××•× 1.0
        
        logger.debug(f"ğŸ¯ ×§×˜×’×•×¨×™×”: {category_name} (×¦×™×•×Ÿ: {score}, ×‘×™×˜×—×•×Ÿ: {confidence:.2f})")
        
        return category_name, confidence
    
    def _detect_topics(self, text: str) -> List[str]:
        """
        ×–×™×”×•×™ × ×•×©××™× ×¨×œ×•×•× ×˜×™×™×
        
        Args:
            text: ×˜×§×¡×˜ ×× ×•×¨××œ
        
        Returns:
            ×¨×©×™××ª × ×•×©××™×
        """
        detected_topics = []
        
        for topic, keywords in self.topic_keywords.items():
            for keyword in keywords:
                if self._word_in_text(keyword, text):
                    detected_topics.append(topic)
                    break  # ××¡×¤×™×§ match ××—×“ ×œ×›×œ × ×•×©×
        
        # ××™×•×Ÿ ×œ×¤×™ ×¡×“×¨ ×—×©×™×‘×•×ª (×œ×¤×™ ×”×’×“×¨×” ×‘-config)
        detected_topics = sorted(
            detected_topics,
            key=lambda t: list(self.topics.keys()).index(t)
        )
        
        logger.debug(f"ğŸ·ï¸ × ×•×©××™× ×©×–×•×”×•: {detected_topics}")
        
        return detected_topics
    
    def _extract_keywords(self, text: str, max_keywords: int = 5) -> List[str]:
        """
        ×—×™×œ×•×¥ ××™×œ×•×ª ××¤×ª×— ××”×˜×§×¡×˜
        
        Args:
            text: ×˜×§×¡×˜ ×× ×•×¨××œ
            max_keywords: ××§×¡×™××•× ××™×œ×•×ª ××¤×ª×—
        
        Returns:
            ×¨×©×™××ª ××™×œ×•×ª ××¤×ª×—
        """
        # ×”×¡×¨×ª ××™×œ×•×ª ×¢×¦×™×¨×” × ×¤×•×¦×•×ª ×‘×¢×‘×¨×™×ª
        stop_words = {
            '××ª', '×©×œ', '×¢×œ', '××œ', '×¢×', '×›×œ', '×œ×', '×–×”', '×”×™×”',
            '××•', '××', '×›×™', '××”', '×™×©', '×¨×§', '×’×', '×× ×™', '×”×•×',
            '×”×™×', '××ª×”', '×”×', '×œ×™', '××‘×œ', '×›×Ÿ', '×œ×•', '×™×•×ª×¨',
            '×¢×•×“', '×¤×”', '×©×', '××–', '×›××•', '×‘×™×Ÿ', '×¤×¢×', '××—×“',
            '×©× ×™', '×›××”', '××—×¨×™', '×œ×¤× ×™', '×ª××™×“', '×¢×›×©×™×•', '×¤×ª××•×'
        }
        
        # ×¤×™×¦×•×œ ×œ××™×œ×™×
        words = re.findall(r'\b\w+\b', text)
        
        # ×¡×™× ×•×Ÿ ××™×œ×•×ª ×¢×¦×™×¨×” ×•××™×œ×™× ×§×¦×¨×•×ª
        keywords = [
            word for word in words
            if word not in stop_words and len(word) > 2
        ]
        
        # ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª ×ª×•×š ×©××™×¨×” ×¢×œ ×¡×“×¨
        seen = set()
        unique_keywords = []
        for word in keywords:
            if word not in seen:
                seen.add(word)
                unique_keywords.append(word)
        
        # ×”×—×–×¨×ª ××§×¡×™××•× X ××™×œ×•×ª ××¤×ª×—
        return unique_keywords[:max_keywords]
    
    def _basic_sentiment_analysis(self, text: str) -> str:
        """
        × ×™×ª×•×— ×¨×’×© ×‘×¡×™×¡×™
        
        Args:
            text: ×˜×§×¡×˜ ×× ×•×¨××œ
        
        Returns:
            'positive', 'negative', 'neutral'
        """
        # ××™×œ×™× ×—×™×•×‘×™×•×ª
        positive_words = [
            '×©××—', '×˜×•×‘', '× ×”×“×¨', '××¦×•×™×Ÿ', '×›×™×£', '××•×”×‘', '××”×‘×ª×™',
            '××¢×•×œ×”', '××“×”×™×', '×™×¤×”', '× ×—××“', '×›×™×™×£', '×’××”', '××œ×•×£',
            '×”×¦×œ×—×”', '××¦×œ×™×—', '×‘×¢×“'
        ]
        
        # ××™×œ×™× ×©×œ×™×œ×™×•×ª
        negative_words = [
            '×¢×¦×•×‘', '×¨×¢', '× ×•×¨×', '×§×©×”', '×›×•××‘', '××¤×—×™×“', '×—×¨×“×”',
            '×œ×—×¥', '××ª×—', '×¢×™×™×£', '×›×¢×¡', '×›×•×¢×¡', '××ª×¡×›×œ', '×‘×¢×™×”',
            '××™×Ÿ ×œ×™ ×›×•×—', '× ×××¡', '×“××’×”', '×“×•××’', '×¤×—×“'
        ]
        
        positive_count = sum(
            1 for word in positive_words
            if self._word_in_text(word, text)
        )
        
        negative_count = sum(
            1 for word in negative_words
            if self._word_in_text(word, text)
        )
        
        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'
    
    def _word_in_text(self, word: str, text: str) -> bool:
        """
        ×‘×“×™×§×” ×× ××™×œ×” ×§×™×™××ª ×‘×˜×§×¡×˜ (×›××™×œ×” ×©×œ××”)
        
        Args:
            word: ×”××™×œ×” ×œ×—×™×¤×•×©
            text: ×”×˜×§×¡×˜ ×œ×—×™×¤×•×© ×‘×•
        
        Returns:
            True ×× ×”××™×œ×” × ××¦××ª
        """
        # ×—×™×¤×•×© ×›××™×œ×” ×©×œ××” (×¢× ×’×‘×•×œ×•×ª ××™×œ×”)
        pattern = r'\b' + re.escape(word) + r'\b'
        return bool(re.search(pattern, text))
    
    def _empty_analysis(self) -> Dict:
        """
        ×ª×•×¦××ª × ×™×ª×•×— ×¨×™×§×”
        
        Returns:
            ××™×œ×•×Ÿ ×¢× ×¢×¨×›×™ ×‘×¨×™×¨×ª ××—×“×œ
        """
        return {
            "category": "×”×¨×”×•×¨×™×",
            "topics": [],
            "keywords": [],
            "sentiment": "neutral",
            "confidence": 0.0
        }
    
    def batch_analyze(self, texts: List[str]) -> List[Dict]:
        """
        × ×™×ª×•×— ×©×œ ××¡×¤×¨ ×˜×§×¡×˜×™× ×‘×‘×ª ××—×ª
        
        Args:
            texts: ×¨×©×™××ª ×˜×§×¡×˜×™×
        
        Returns:
            ×¨×©×™××ª ×ª×•×¦××•×ª × ×™×ª×•×—
        """
        return [self.analyze(text) for text in texts]
    
    def get_category_emoji(self, category: str) -> str:
        """
        ×§×‘×œ×ª ×”××™××•×’'×™ ×©×œ ×§×˜×’×•×¨×™×”
        
        Args:
            category: ×©× ×”×§×˜×’×•×¨×™×”
        
        Returns:
            ×”××™××•×’'×™
        """
        return self.categories.get(category, {}).get("emoji", "ğŸ“")
    
    def get_topic_emoji(self, topic: str) -> str:
        """
        ×§×‘×œ×ª ×”××™××•×’'×™ ×©×œ × ×•×©×
        
        Args:
            topic: ×©× ×”× ×•×©×
        
        Returns:
            ×”××™××•×’'×™
        """
        return self.topics.get(topic, {}).get("emoji", "ğŸ·ï¸")
    
    def format_analysis_summary(self, analysis: Dict, text: str) -> str:
        """
        ×™×¦×™×¨×ª ×¡×™×›×•× ××¤×•×¨××˜ ×©×œ ×”× ×™×ª×•×—
        
        Args:
            analysis: ×ª×•×¦××•×ª ×”× ×™×ª×•×—
            text: ×”×˜×§×¡×˜ ×”××§×•×¨×™
        
        Returns:
            ××—×¨×•×–×ª ××¤×•×¨××˜×ª
        """
        category = analysis.get("category", "×”×¨×”×•×¨×™×")
        topics = analysis.get("topics", [])
        keywords = analysis.get("keywords", [])
        confidence = analysis.get("confidence", 0.0)
        
        # ××™××•×’'×™ ×§×˜×’×•×¨×™×”
        category_emoji = self.get_category_emoji(category)
        
        summary_lines = [
            f"{category_emoji} *×§×˜×’×•×¨×™×”:* {category}",
        ]
        
        # × ×•×©××™×
        if topics:
            topics_str = ", ".join([
                f"{self.get_topic_emoji(t)} {t}"
                for t in topics
            ])
            summary_lines.append(f"*× ×•×©××™×:* {topics_str}")
        
        # ××™×œ×•×ª ××¤×ª×—
        if keywords:
            keywords_str = ", ".join([f"#{kw}" for kw in keywords[:3]])
            summary_lines.append(f"*×ª×’×™×•×ª:* {keywords_str}")
        
        # ×‘×™×˜×—×•×Ÿ (×¨×§ ×× × ××•×š)
        if confidence < 0.5:
            summary_lines.append(f"_×”×¢×¨×”: ×œ× ×‘×˜×•×— ×œ×’××¨×™ ×‘×¡×™×•×•×’_")
        
        return "\n".join(summary_lines)


# ×™×¦×™×¨×ª ××•×‘×™×™×§×˜ ×’×œ×•×‘×œ×™
nlp = NLPAnalyzer()
